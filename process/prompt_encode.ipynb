{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading packages'''\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModel,\n",
    "                          AutoConfig,\n",
    "                          BertTokenizer,\n",
    "                          BertModel\n",
    "                         )\n",
    "\n",
    "def loadBert(device, language_model, num_labels):\n",
    "    print(language_model, 'is used as the language model.')\n",
    "    assert language_model != None, \"language_model is None.\"\n",
    "    if language_model == 'BioBert':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        BioBert=AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    elif language_model ==\"bioRoberta\":\n",
    "        config = AutoConfig.from_pretrained(\"allenai/biomed_roberta_base\", num_labels = num_labels)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"allenai/biomed_roberta_base\")\n",
    "        BioBert = AutoModel.from_pretrained(\"allenai/biomed_roberta_base\")\n",
    "    elif  language_model == \"Bert\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        BioBert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    elif language_model == \"bioLongformer\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "        BioBert= AutoModel.from_pretrained(\"yikuan8/Clinical-Longformer\")\n",
    "    elif language_model == \"ClinicalBERT\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "        BioBert = AutoModel.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "    else:\n",
    "        raise ValueError(\"language_model should be BioBert, bioRoberta, bioLongformer, ClinicalBERT or Bert\")\n",
    "    \n",
    "    for param in BioBert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    BioBert = BioBert.to(device)\n",
    "    return BioBert, tokenizer\n",
    "\n",
    "'''Frozen Text Representation'''\n",
    "class BertForRepresentation(nn.Module):\n",
    "    def __init__(self, BioBert, language_modelname):\n",
    "        super().__init__()\n",
    "        self.bert = BioBert\n",
    "        self.language_model = language_modelname\n",
    "        if self.language_model in ['ClinicalBERT']:\n",
    "            self.dropout = torch.nn.Dropout(BioBert.config.dropout)\n",
    "        else:\n",
    "            self.dropout = torch.nn.Dropout(BioBert.config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids_sequence, attention_mask_sequence):\n",
    "        txt_arr = []\n",
    "        for input_ids, attention_mask  in zip(input_ids_sequence, attention_mask_sequence):\n",
    "            text_embeddings=self.bert(input_ids, attention_mask=attention_mask)\n",
    "            text_embeddings= text_embeddings[0][:,0,:]\n",
    "            text_embeddings = self.dropout(text_embeddings)\n",
    "            txt_arr.append(text_embeddings)\n",
    "        return torch.stack(txt_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClinicalBERT is used as the language model.\n"
     ]
    }
   ],
   "source": [
    "'''Checking if GPU is available'''\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "language_model = \"ClinicalBERT\"\n",
    "BioBert, tokenizer = loadBert(device, language_model, num_labels = 2)\n",
    "text_extractor = BertForRepresentation(BioBert, language_model).to(device)\n",
    "\n",
    "def template_representations(templates, text_extractor, device):\n",
    "    text_token=[]\n",
    "    attention_mask=[]\n",
    "    for template in templates:\n",
    "        template_codes = tokenizer(template, \n",
    "                                   padding=True,\n",
    "                                   max_length=512,\n",
    "                                   add_special_tokens=True,\n",
    "                                   return_attention_mask = True,\n",
    "                                   truncation=True)\n",
    "        text_token.append(torch.tensor(template_codes['input_ids'], dtype=torch.long))\n",
    "        attention_mask.append(torch.tensor(template_codes['attention_mask'], dtype=torch.long))\n",
    "    \n",
    "    print(text_token[0], attention_mask[0].shape)\n",
    "    text_token, attention_mask = padding(text_token, attention_mask, max_length = 512)\n",
    "    \n",
    "    text_token = text_token.unsqueeze(dim=0)\n",
    "    attention_mask = attention_mask.unsqueeze(dim=0)\n",
    "\n",
    "    text_token = text_token.clone().detach().to(device, dtype=torch.long)\n",
    "    attention_mask = attention_mask.clone().detach().to(device, dtype=torch.long)\n",
    "    return text_token, attention_mask, text_extractor(text_token, attention_mask)\n",
    "\n",
    "def padding(text_token, atten_mask, max_length):\n",
    "    text_token = pad_sequence(text_token, batch_first=True, padding_value=0) # dim_token * num_note \n",
    "    atten_mask = pad_sequence(atten_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    print(text_token.shape)\n",
    "    text_token = torch.nn.functional.pad(text_token, (0, max_length - text_token.size(1)), value=0)\n",
    "    atten_mask = torch.nn.functional.pad(atten_mask, (0, max_length - atten_mask.size(1)), value=0)\n",
    "    return text_token, atten_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 10105, 38607, 10124, 40345, 10114, 10105, 18141, 10662, 10151,\n",
      "        44461, 27541,   117, 54188, 10114, 13596,   122,   119,   102]) torch.Size([19])\n",
      "torch.Size([3, 19])\n"
     ]
    }
   ],
   "source": [
    "'''Based on previous work, we extract the task template using hard-encoding templates'''\n",
    "templates1 = ['The patient is admitted to the hospital after an emergency visit, belonging to class 1.', \\\n",
    "             'The patient did not require hospitalization after the emergency visit, belonging to class 0.', \\\n",
    "             'The patient [mask] after the emergency visit, belonging to class unknown.']\n",
    "\n",
    "text_token, attention_mask, template_representations = template_representations(templates1, text_extractor, device)\n",
    "with open('./config/task1_template.pkl', 'wb') as f:\n",
    "    pickle.dump(template_representations, f)\n",
    "\n",
    "# import pickle\n",
    "# text = pickle.load(open('./config/task1_template.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Based on previous work, we extract the task template using hard-encoding templates'''\n",
    "templates2 = ['The patient died during their hospitalization or was urgently transferred to the Intensive Care Unit (ICU) within 12 hours, belonging to class 1.', \\\n",
    "             'The patient remained alive throughout their hospitalization and did not undergo an emergency transfer to the Intensive Care Unit (ICU) within 12 hours, belonging to class 0.', \\\n",
    "             'The patient [mask] throughout their hospitalization and [mask] to the Intensive Care Unit (ICU) within 12 hours, belonging to class unknown.']\n",
    "\n",
    "text_token2, attention_mask2, template_representations2 = template_representations(templates2, text_extractor, device)\n",
    "with open('./config/task2_template.pkl', 'wb') as f:\n",
    "    pickle.dump(template_representations2, f)\n",
    "\n",
    "import pickle\n",
    "text = pickle.load(open('./config/task2_template.pkl', \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
